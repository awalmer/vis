library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(plyr)
# Data wrapper location matching:
rename_location <- function(data_set, old_name, new_name) {
michelin_restaurants <-
data_set %>%
mutate(across('location', str_replace, old_name, new_name))
return(michelin_restaurants)
}
# Scrape Michelin data:
scrape_michelin_data <- function(how_many_stars, number_pages_in_search) {
# Build the URL:
url_part1 <- "https://guide.michelin.com/us/en/restaurants/"
if (how_many_stars == 1) {
url_part2 <- "-star-michelin/page/" # singular
} else {
url_part2 <- "-stars-michelin/page/" # plural
}
# Initial empty data frame:
#michelin_df <- data.frame("title"="","location"="","price_cuisine"="")
michelin_df <- data.frame(matrix(NA, ncol = 3, nrow = 0))
# colnames(michelin_df) <- c("title", "location", "price_cuisine")
# For each result page, scrape restaurant data (loop):
for (p in 1:number_pages_in_search) {
# Search result URL:
search_result_url <- paste0(url_part1, how_many_stars, url_part2, p, "?")
# Note: Adding "?" at end of URL stabilizes URL location
# Read html:
html_doc <- read_html(search_result_url)
# Capture restaurant titles:
title <- as.character(
html_doc %>%
html_elements("h3.card__menu-content--title") %>%
html_elements("a") %>%
html_text2()
)
# Capture restaurant locations, price tier, and cuisine:
# Note: as of 2024, HTML elements of Michelin website changed.
lpc_vector <- as.vector(
html_doc %>%
html_elements("div.align-items-end") %>%
html_text2())[2:21] # provides "\n" separated character block
lpc_table <- tibble(lpc_vector) # convert to table for extraction
lpc_table_separated <- lpc_table %>%
separate_wider_delim(lpc_vector, "\n", names = c("location", "price_cuisine"))
# Append elements to Michelin data frame:
michelin_df_append <-
data.frame("title"=title,
"location"=lpc_table_separated$location,
"price_cuisine"=lpc_table_separated$price_cuisine)
michelin_df <- rbind(michelin_df, michelin_df_append)
} # end loop
# Clean / reformat data frame & add new variables:
michelin_df$price <- str_split_i(michelin_df$price_cuisine, " . ", 1)
michelin_df$cuisine <- str_split_i(michelin_df$price_cuisine, " . ", 2)
michelin_df$price_cuisine <- NULL
michelin_df$michelin_stars <- how_many_stars
# Sometimes the location is city + country, or just country.
# Create country variable from conditional comma split:
michelin_df$country <-
ifelse(grepl(", ", michelin_df$location), # condition
str_split_i(michelin_df$location, ", ", 2), # case if condition true
michelin_df$location # case if condition false (no comma)
)
# Create distinction variable in character format for labeling:
michelin_df <-
michelin_df %>%
mutate(distinction = case_when(
michelin_stars == 1 ~ "1 Star",
michelin_stars == 2 ~ "2 Stars",
michelin_stars == 3 ~ "3 Stars"))
# Rename certain countries:
michelin_df <- rename_location(michelin_df, 'China Mainland', 'China')
michelin_df <- rename_location(michelin_df, 'Leynavatn, Denmark', 'Leynavatn')
# Return data frame:
return(michelin_df)
}
# Test:
test <- scrape_michelin_data(1,1)
library(jsonlite)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE,paged.print=FALSE,fig.width = 8)
library(readxl)
babynames_url <- "https://raw.githubusercontent.com/ireapps/teaching-guide-R123/main/data/babynames_excel.xlsx"
# Download the file
# Use the download.file function to download the file from the specified URL
# - url: the URL of the file
# - destfile: the destination file path where the downloaded file will be saved
# - mode: the mode for file transfer; "wb" stands for "write binary", which is used for binary files
download.file(babynames_url, destfile = "babynames_excel.xlsx", mode = "wb")
# Read the downloaded file
babynames <- read_xlsx("babynames_excel.xlsx")
View(babynames)
glimpse(babynames)
babynamesa <- read_xlsx("babynames_excel.xlsx", sheet = "babynames_a")
babynamesa <- read_xlsx("babynames_excel.xlsx", sheet = "babynames_a")
babynamesb <- read_xlsx("babynames_excel.xlsx", sheet = "babynames_b")
View(babynamesa)
View(babynamesb)
ts_url <- "https://en.wikipedia.org/wiki/List_of_Taylor_Swift_live_performances"
ts_wiki <- read_html(ts_url)
library(rvest)
ts_url <- "https://en.wikipedia.org/wiki/List_of_Taylor_Swift_live_performances"
ts_wiki <- read_html(ts_url)
View(ts_wiki)
# Reading the second table from the webpage
taylor_live_tours <- read_html(ts_url) %>%
html_table()
taylor_live_tours <- read_html(ts_url) %>%
html_table()
taylor_live_tours <- taylor_live_tours[[2]]
View(taylor_live_tours)
install.packages("taylor")
# install "taylor" library
library(taylor)
glimpse(albums)
albums <- taylor::taylor_albums
glimpse(albums)
sessions <- fromJSON("https://schedules.ire.org/ire-2024/ire-2024-schedule.json")
str(sessions, max.level = 1) #we use max.level to limit the depth of the output since we have nested dataframes and lists inside the overall dataframe, a function of the JSON file
library(jsonlite)
View(sessions)
sessions <- fromJSON(txt = "https://schedules.ire.org/ire-2024/ire-2024-schedule.json")
View(sessions)
sessions <- sessions %>%
mutate(
track = as_factor(track),
session_type = as.factor(session_type),
evergreen = as.logical(evergreen),
canceled = as.logical(canceled),
recorded = as.logical(recorded),
day = fct_relevel(as_factor(day), "Thursday", "Friday", "Saturday", "Sunday"), # using fct_relevel to specify the order
start_time_utc = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
end_time_utc = as.POSIXct(end_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
cost = as.numeric(cost),
start_time_example = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%sZ", tz = "UTC")
)
View(sessions)
sessions <- sessions %>%
mutate(
track = as_factor(track),
session_type = as.factor(session_type),
evergreen = as.logical(evergreen),
canceled = as.logical(canceled),
recorded = as.logical(recorded),
day = fct_relevel(as_factor(day), "Thursday", "Friday", "Saturday", "Sunday"), # using fct_relevel to specify the order
start_time_utc = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
end_time_utc = as.POSIXct(end_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
cost = as.numeric(cost),
start_time_example = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
)
str(sessions,max.level=1) #checking to see if the changes were made
sessions <- sessions %>%
mutate(
start_time_pt = with_tz(start_time_utc, tzone = "America/Los_Angeles"),
end_time_pt = with_tz(end_time_utc, tzone = "America/Los_Angeles")
)
library(lubridate)
lubridate::ymd_hms
library(lubridate)
lubridate::ymd_hms
sessions <- sessions %>%
mutate(
start_time_pt = with_tz(start_time_utc, tzone = "America/Los_Angeles"),
end_time_pt = with_tz(end_time_utc, tzone = "America/Los_Angeles"),
duration_2 = as.duration(end_time_pt-start_time_pt)
)
str(sessions,max.level=1)
sessions <- sessions %>%
select(-start_time_utc, -end_time_utc,-start_time,-end_time) %>%
rename(start_time = start_time_pt, end_time = end_time_pt) #renaming the columns to reflect the new time zone
sessions <- sessions %>%
select(-start_time_utc, -end_time_utc,-start_time,-end_time) %>%
rename(start_time = start_time_pt, end_time = end_time_pt) #renaming the columns to reflect the new time zone
sessions <- sessions %>%
mutate(
start_time_pt = with_tz(start_time_utc, tzone = "America/Los_Angeles"),
end_time_pt = with_tz(end_time_utc, tzone = "America/Los_Angeles"),
duration_2 = as.duration(end_time_pt-start_time_pt)
)
sessions <- sessions %>%
mutate(
track = as_factor(track),
session_type = as.factor(session_type),
evergreen = as.logical(evergreen),
canceled = as.logical(canceled),
recorded = as.logical(recorded),
day = fct_relevel(as_factor(day), "Thursday", "Friday", "Saturday", "Sunday"), # using fct_relevel to specify the order
start_time_utc = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
end_time_utc = as.POSIXct(end_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
cost = as.numeric(cost),
start_time_example = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
)
str(sessions,max.level=1) #checking to see if the changes were made
library(lubridate)
lubridate::ymd_hms
sessions <- sessions %>%
mutate(
start_time_pt = with_tz(start_time_utc, tzone = "America/Los_Angeles"),
end_time_pt = with_tz(end_time_utc, tzone = "America/Los_Angeles"),
duration_2 = as.duration(end_time_pt-start_time_pt)
)
str(sessions,max.level=1)
sessions <- sessions %>%
select(-start_time_utc, -end_time_utc,-start_time,-end_time) %>%
rename(start_time = start_time_pt, end_time = end_time_pt) #renaming the columns to reflect the new time zone
str(sessions,max.level=1)
sessions %>% count(recorded) #counting the number of TRUE and FALSE values in the canceled column
not_recorded <- sessions %>% filter(recorded == "FALSE")
future_not_recorded <- not_recorded %>% filter(day %in% c("Saturday", "Sunday")) #filtering for the future sessions that won't be recorded
#Or if we want to find future not-recorded sessions in a programmatic way that will work at any point in time
future_not_recorded <- not_recorded %>% filter(start_time > Sys.time())
ideal_sessions <- future_not_recorded %>% filter(evergreen == FALSE & canceled == FALSE & !grepl("registration",session_type,ignore.case=TRUE) & (grepl("Edit",track,ignore.case=TRUE) | grepl("manag",track,ignore.case=TRUE) | grepl("writing",track,ignore.case=TRUE) | track=="" | grepl("elect",track,ignore.case=TRUE)))
ideal_handson <- ideal_sessions %>% filter(session_type == "Hands-on")
sessions <- session %>%
filter(canceled == FALSE) %>%
filter(grepl("data",track,ignore.case=TRUE))
sessions <- sessions %>%
filter(canceled == FALSE) %>%
filter(grepl("data",track,ignore.case=TRUE))
View(sessions)
sessions <- sessions %>%
filter(canceled == FALSE) %>%
filter(grepl("data",track,ignore.case=TRUE)) %>%
filter(recorded == FALSE)
sessions <- sessions %>%
mutate(
track = as_factor(track),
session_type = as.factor(session_type),
evergreen = as.logical(evergreen),
canceled = as.logical(canceled),
recorded = as.logical(recorded),
day = fct_relevel(as_factor(day), "Thursday", "Friday", "Saturday", "Sunday"), # using fct_relevel to specify the order
start_time_utc = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
end_time_utc = as.POSIXct(end_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
cost = as.numeric(cost),
start_time_example = as.POSIXct(start_time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
)
str(sessions,max.level=1) #checking to see if the changes were made
We see that the start time in the original data, and in the cleaned data, is in UTC. We can convert this to the local time zone of the conference, which is Pacific Time.
```{r}
library(lubridate)
lubridate::ymd_hms
sessions <- sessions %>%
mutate(
start_time_pt = with_tz(start_time_utc, tzone = "America/Los_Angeles"),
end_time_pt = with_tz(end_time_utc, tzone = "America/Los_Angeles"),
duration_2 = as.duration(end_time_pt-start_time_pt)
)
str(sessions,max.level=1)
sessions <- sessions %>%
select(-start_time_utc, -end_time_utc,-start_time,-end_time) %>%
rename(start_time = start_time_pt, end_time = end_time_pt) #renaming the columns to reflect the new time zone
str(sessions,max.level=1)
sessions %>% count(recorded) #counting the number of TRUE and FALSE values in the canceled column
not_recorded <- sessions %>% filter(recorded == "FALSE")
future_not_recorded <- not_recorded %>% filter(day %in% c("Saturday", "Sunday")) #filtering for the future sessions that won't be recorded
#Or if we want to find future not-recorded sessions in a programmatic way that will work at any point in time
future_not_recorded <- not_recorded %>% filter(start_time > Sys.time())
ideal_sessions <- future_not_recorded %>% filter(evergreen == FALSE & canceled == FALSE & !grepl("registration",session_type,ignore.case=TRUE) & (grepl("Edit",track,ignore.case=TRUE) | grepl("manag",track,ignore.case=TRUE) | grepl("writing",track,ignore.case=TRUE) | track=="" | grepl("elect",track,ignore.case=TRUE)))
ideal_handson <- ideal_sessions %>% filter(session_type == "Hands-on")
sessions <- sessions %>%
filter(canceled == FALSE) %>%
filter(grepl("data",track,ignore.case=TRUE)) %>%
filter(recorded == FALSE)
glimpse(sessions)
sessions %>% count(recorded) #counting the number of TRUE and FALSE values in the canceled column
not_recorded <- sessions %>% filter(recorded == "FALSE")
future_not_recorded <- not_recorded %>% filter(day %in% c("Saturday", "Sunday")) #filtering for the future sessions that won't be recorded
ideal_sessions <- future_not_recorded %>% filter(evergreen == FALSE & canceled == FALSE & !grepl("registration",session_type,ignore.case=TRUE) & (grepl("Edit",track,ignore.case=TRUE) | grepl("manag",track,ignore.case=TRUE) | grepl("writing",track,ignore.case=TRUE) | track=="" | grepl("elect",track,ignore.case=TRUE)))
glimpse(ideal_sessions)
str(sessions$speakers[[1]],max.level=1)
all_speakers1 <- bind_rows(sessions$speakers) #binding all the speakers into one dataframe
glimpse(all_speakers1)
View(all_speakers1)
all_speakers2 <- sessions %>%
unnest(cols = speakers) #unnesting the speakers column
glimpse(all_speakers2)
View(all_speakers2)
all_speakers <- sessions %>%
select(session_id, session_title, speakers) %>%
unnest(cols=speakers)
glimpse(all_speakers)
View(all_speakers)
all_speakers %>%
count(name, sort = TRUE) #counting the number of sessions each speaker is in
#paste option
all_speakers1 <- all_speakers %>%
mutate(name = paste(first, last)) %>%
select(-first, -last) #combining the first and last name into one column
#unite option
all_speakers2 <- all_speakers %>%
unite(name, first, last, sep = " ", remove = FALSE) #combining the first and last name into one column with a space between them.
#lets just keep the paste option.
all_speakers <- all_speakers1
rm(all_speakers1,all_speakers2) #removing the other dataframes
all_speakers %>%
count(name, sort = TRUE) #counting the number of sessions each speaker is in
all_speakers %>%
count(name) %>%
count(n) #counting the number of speakers who are speaking once versus those speaking more than once
all_speakers %>%
count(name, sort = TRUE) #counting the number of sessions each speaker is in
library(devtools)
install.packages("devtools") # if not already installed on your system
devtools::install_github("munichrocker/DatawRappr")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools") # if not already installed on your system
devtools::install_github("munichrocker/DatawRappr")
library(devtools)
install.packages("devtools") # if not already installed on your system
install.packages("devtools")
library(devtools)
devtools::install_github("munichrocker/DatawRappr")
install.packages("devtools")
devtools::install_github("munichrocker/DatawRappr")
update.packages("rlang")
install.packages("devtools")
library(devtools)
packageVersion("rlang") # ‘1.0.6’
library(devtools)
remove.packages("rlang")
library(devtools)
packageVersion("rlang") # ‘1.0.6’
update.packages("rlang")
packageVersion("rlang")
install.packages("rlang")
install.packages("rlang")
packageVersion("rlang")
devtools::install_github("munichrocker/DatawRappr")
devtools::install_github("munichrocker/DatawRappr")
library(DatawRappr)
library(tidyverse)
datawrapper_auth(api_key =  "pgo0RQAWCOfL4vJeturiHaqzO1iQLgYqEEHvNMvZcveUVpTXww55FL0878G0Y1yP", overwrite=TRUE)
crime_data <- read.csv("mn-crime-data.csv")
head(crime_data)
View(crime_data)
crime_data$Year <- year(crime_data$Reported_Date)
crime_data <- crime_data %>%
filter(Year < 2025)
crime_summary <- crime_data %>%
count(Year)
nrow(crime_data)
sum(crime_summary$n)
crime_summary <- crime_summary %>%
rename("Reported crimes" = "n")
View(crime_data)
View(crime_summary)
my_chart <- dw_create_chart()
dw_data_to_chart(crime_summary, my_chart)
dw_edit_chart(my_chart, title = "Crime in Minneapolis hit a three-year low in 2024",
intro = "Reported crimes in the city between 2019 and 2024 hit a high mark in 2022 but have been on the decline since.", source_name = "Open Data Minneapolis")
dw_publish_chart(my_chart)
png_chart <- dw_export_chart(my_chart, type = "png")
magick::image_write(png_chart, "line-chart.png")
It should look like this:
bar_chart <- dw_create_chart(
type="d3-bars",
title="Minneapolis reported crime, 2019-2024"
)
dw_data_to_chart(crime_summary, bar_chart)
dw_edit_chart(bar_chart, title = "Crime in Minneapolis hit a three-year low in 2024",
intro = "Reported crimes in the city between 2019 and 2024 hit a high mark in 2022 but have been on the decline since.", source_name = "Open Data Minneapolis")
dw_retrieve_chart_metadata(bar_chart)
dw_edit_chart(bar_chart,
byline = "Adam Marton",
visualize = list(
"base-color" = "#a47764",
"thick" = "true",
"value-label-alignment" = "right"
)
)
dw_publish_chart(bar_chart)
crimes_2024 <- crime_data %>%
filter(Year == 2024)
crime_offense_summary <- crimes_2024 %>%
count(Offense_Category)
crime_offense_summary <- crime_offense_summary %>%
arrange(desc(n)) %>%
slice(1:10)
offense_chart <- dw_copy_chart(copy_from = "INSERT CHART ID FROM BAR CHART ABOVE")
offense_chart <- dw_copy_chart(copy_from = "INSERT CHART ID FROM BAR CHART ABOVE")
crime_offense_summary <- crime_offense_summary %>%
arrange(desc(n)) %>%
slice(1:10)
crime_offense_summary <- crime_offense_summary %>%
arrange(desc(n)) %>%
slice(1:10)
crime_offense_summary <- crimes_2024 %>%
count(Offense_Category)
crime_offense_summary <- crime_offense_summary %>%
arrange(desc(n)) %>%
slice(1:10)
dw_publish_chart(offense_chart)
offense_chart <- dw_copy_chart(copy_from = "U9ABN")
dw_data_to_chart(crime_offense_summary, offense_chart)
dw_edit_chart(offense_chart, title = "Top 10 highest categories of crime offenses in Minneapolis in 2024",
intro = "Almost 13,000 larceny and theft offenses and over 8,000 assault offenses were reported.", source_name = "Open Data Minneapolis")
dw_publish_chart(offense_chart)
dw_retrieve_chart_metadata(offense_chart)
dw_edit_chart(offense_chart,
visualize = list(
"base-color" = "#8ACE00",
"sort-bars" = "true",
"rules" = "true",
"background" = "true",
"block-labels" = "true",
"value-label-alignment" = "right"
)
)
dw_publish_chart(offense_chart)
crime_neighborhood_summary <- crimes_2024 %>%
count(Neighborhood)
crime_neighborhood_summary <- crime_neighborhood_summary %>%
arrange(desc(n)) %>%
slice(1:10)
my_function <- function() {
#make a new data frame that holds data just for precinct 1
precinct_data <- crime_data %>% filter(Precinct== 1)
#count number of arrests per year in that precinct
precinct_summary <- precinct_data %>%count(Year)
#create new chart
my_chart <- dw_create_chart(
type="d3-bars",
)
#add data
dw_data_to_chart(precinct_summary, my_chart)
#edit chart
dw_edit_chart(my_chart, title = paste("Reported crimes by year in Minneapolis Precinct 1"), source_name = "Open Data Minneapolis")
#publish
dw_publish_chart(my_chart)
}
#Call the function to run it
my_function()
#add a parameter called precinct
my_function <- function(precinct) {
#instead of naming the precinct here, use the parameter
precinct_data <- crime_data %>% filter(Precinct == precinct)
precinct_summary <- precinct_data %>%count(Year)
my_chart <- dw_create_chart(
type="d3-bars",
)
dw_data_to_chart(precinct_summary, my_chart)
#Again, use the parameter instead of naming the specific precinct in the headline
dw_edit_chart(my_chart, title = paste("Reported crimes by year in Minneapolis Precinct", precinct), source_name = "Open Data Minneapolis")
dw_publish_chart(my_chart)
}
#Call the function and pass an argument -- "2"-- to the parameter
#This passes the 2 everywhere we have the parameter in the function, so our graphic will display crimes in precinct 2
my_function("2")
for(precinct in unique(crime_data$Precinct)) {
my_function(precinct)
}
pixar_films <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')
public_response <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')
View(pixar_films)
View(public_response)
join()
join
View(pixar_films)
source("~/Documents/github-vis/vis/src/passport-data-loader.R", echo=TRUE)
View(rank_by_year)
View(europe)
# Oceania:
oceania <- filterByRegion(rank_by_year, "OCEANIA")
oceania <- cleanRegionData(oceania, replace_0="yes", remove_0709="yes")
## Export to JSON:
region_df_list <- list("asia"=asia, "europe"=europe, "africa"=africa,
"caribbean"=caribbean, "oceania"=oceania)
dfWriteJSON(region_df_list)
# Middle East:
mideast <- filterByRegion(rank_by_year, "MIDDLE EAST")
mideast <- cleanRegionData(mideast, replace_0="yes", remove_0709="yes")
## Export to JSON:
region_df_list <- list("asia"=asia, "europe"=europe, "africa"=africa,
"caribbean"=caribbean, "oceania"=oceania,
"mideast"=mideast)
dfWriteJSON(region_df_list)
mideast_names_old <- list('United Arab Emirates','Palestinian Territory')
mideast_names_new <- list('Utd. Arab Emirates','Palestinian Terr.')
mideast <- replaceNames(mideast, 2, mideast_names_old, mideast_names_new)
## Export to JSON:
region_df_list <- list("asia"=asia, "europe"=europe, "africa"=africa,
"caribbean"=caribbean, "oceania"=oceania,
"mideast"=mideast)
dfWriteJSON(region_df_list)
# Middle East:
mideast <- filterByRegion(rank_by_year, "MIDDLE EAST")
mideast <- cleanRegionData(mideast, replace_0="yes", remove_0709="yes")
mideast_names_old <- list('United Arab Emirates','Palestinian Territory')
mideast_names_new <- list('Utd. Arab Emirates','Palestinian Territ.')
mideast <- replaceNames(mideast, 2, mideast_names_old, mideast_names_new)
## Export to JSON:
region_df_list <- list("asia"=asia, "europe"=europe, "africa"=africa,
"caribbean"=caribbean, "oceania"=oceania,
"mideast"=mideast)
dfWriteJSON(region_df_list)
# Americas:
americas <- filterByRegion(rank_by_year, "AMERICAS")
americas <- cleanRegionData(americas, replace_0="yes", remove_0709="yes")
# Americas:
americas <- filterByRegion(rank_by_year, "AMERICAS")
americas <- cleanRegionData(americas, replace_0="yes", remove_0709="yes")
## Export to JSON:
region_df_list <- list("asia"=asia, "europe"=europe, "africa"=africa,
"caribbean"=caribbean, "oceania"=oceania,
"mideast"=mideast, "americas"=americas)
dfWriteJSON(region_df_list)
